{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Approach: Re-sampling with SMOTE\n",
    "\n",
    "In this approach, we will use the **SMOTE (Synthetic Minority Over-sampling Technique)** algorithm to balance the dataset. SMOTE generates synthetic samples for the minority class to balance the dataset. First, we will oversample the minority class **Enrolled**, then we will train a model using the balanced dataset. Additionally, we will perform other experiments, such as oversampling the other minority class **Dropout** and undersampling the majority class **Graduated**.\n",
    "\n",
    "The SMOTE algorithm is described in the following paper: [SMOTE: Synthetic Minority Over-sampling Technique](https://www.jair.org/index.php/jair/article/view/10302/24590). In this paper, the authors present the algorithm and demonstrate how this technique of oversampling the minority class is superior to traditional oversampling techniques, which simply duplicate the samples of the minority class.\n",
    "\n",
    "Below is the pseudo-code for a two-class problem:\n",
    "\n",
    "```julia\n",
    "Algorithm SMOTE(T, N, k)\n",
    "Input:\n",
    "    T = Number of minority class samples\n",
    "    N = Percentage of oversampling (SMOTE percentage)\n",
    "    k = Number of nearest neighbors\n",
    "\n",
    "Output:\n",
    "    (N/100) * T synthetic minority class samples\n",
    "\n",
    "1. If N is less than 100%, randomize the minority class samples, as only a random percentage of them will be SMOTEd.\n",
    "2. If N < 100 then\n",
    "    3. Randomize the T minority class samples\n",
    "    4. T = (N / 100) * T\n",
    "    5. N = 100\n",
    "6. End if\n",
    "7. N = (int)(N / 100) * T  (*The amount of SMOTE is assumed to be in integral multiples of 100.*)\n",
    "8. k = Number of nearest neighbors\n",
    "9. numattrs = Number of attributes\n",
    "10. Sample[][]: Array for original minority class samples\n",
    "11. newindex: Counter for number of synthetic samples, initialized to 0\n",
    "12. Synthetic[][]: Array for synthetic samples\n",
    "\n",
    "13. For i = 1 to T\n",
    "    14. Compute k nearest neighbors for sample i, and save the indices in nnarray\n",
    "    15. Populate(N, i, nnarray)\n",
    "16. End for\n",
    "\n",
    "Function Populate(N, i, nnarray):\n",
    "17. While N > 0\n",
    "    18. Choose a random number between 1 and k, call it nn. This step selects one of the k nearest neighbors of sample i.\n",
    "    19. For each attribute (attr) from 1 to numattrs:\n",
    "        20. Compute the difference: dif = Sample[nnarray[nn]][attr] - Sample[i][attr]\n",
    "        21. Compute a random gap: gap = random number between 0 and 1\n",
    "        22. Synthetic[newindex][attr] = Sample[i][attr] + gap * dif\n",
    "    20. End for\n",
    "    23. Increment newindex\n",
    "    24. Decrement N\n",
    "25. End while\n",
    "\n",
    "26. Return synthetic samples\n",
    "End of pseudo-code.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the used models\n",
    "\n",
    "To be able to compare the results with the first and second approaches, we will use the same configuration of hyperparameters for the models:\n",
    "\n",
    "- **ANN**:\n",
    "  - Hidden layers: 1, number of neurons in the hidden layer: $[16, 32, 64]$.\n",
    "  - Hidden layers: 2, number of neurons in the hidden layers $[(16, 16), (32, 16), (32, 32), (64, 32), (64, 64)]$.\n",
    "- **Decision Tree**:\n",
    "  - Maximum depth of the tree $\\in \\{3, 5, 10, 15, 20, \\text{None}\\}$\n",
    "- **SVM**:\n",
    "  - Kernel $\\in \\{\\text{linear}, \\text{poly}, \\text{rbf}, \\text{sigmoid}\\}$\n",
    "  - C $\\in \\{0.1, 1, 10\\}$\n",
    "- **KNN\\***:\n",
    "  - $k \\in \\{1, 3, 5, 7, 9, 11, 13, 15\\}$\n",
    "\n",
    "After training the models, we will train an ensemble model with the three best models. The method used to combine the models will be:\n",
    "\n",
    "- **Majority voting**\n",
    "- **Weighted voting**\n",
    "- **Naive Bayes**\n",
    "- **Stacking** (using a logistic regression as the meta-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imbalanced-learn (revision): https://www.sciencedirect.com/science/article/pii/S0957417416307175?casa_token=lyglFt_Ye0YAAAAA:Apv_dixqX-GQm04rHLrN6wBhIRJHhxCFlqUS5WXXbuD-iJCO9FUBZ9VLAxgRDwUTKdpPTGgHIA\n",
    "  - Aqui falan de SMOTE (tecnica para over-sampling), tecnicas de re-sampling híbridas e undersampling.\n",
    "  - Tamén falan de que aplicando PCA e tecnicas de reducción da dimensionalidade se pode reducir o efecto negativo do desbalanceamento.\n",
    "  - Tamén comentan que as técnicas de ensamblado se utilizan para estas situacións, pero me da que non vai poder ser aplicable ao noso caso (utilizan AdaBoost e esas vainas).\n",
    "  - Por último, comentan que se poden utilizar técnicas para ponderar os erros. Guai se utilizamos unha rede neuronal, posto que so hai que cambiar a función de custo, pero implementa ti isto en SVM ou Decision Tree. É posible, de feito, hai formas e explicanse, pero implicaría cambiar o codigo drasticamente.\n",
    "- Learning from imbalanced data (non o lin, pero creo que tamén describe técnicas de resampling): https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5128907\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index**\n",
    "\n",
    "- [Data loading](#Data-loading)\n",
    "- [SMOTE configurations](#SMOTE-experiments)\n",
    "- [Individual models](#Individual-models)\n",
    "  - [ANN](#ANN)\n",
    "  - [Decision Tree](#Decision-Tree)\n",
    "  - [Support Vector Machine](#Support-Vector-Machine)\n",
    "  - [K-Nearest Neighbors](#K-Nearest-Neighbors)\n",
    "- [Ensemble model](#Ensemble-model)\n",
    "  - [Majority voting](#Majority-voting)\n",
    "  - [Weighted voting](#Weighted-voting)\n",
    "  - [Naive Bayes](#Naive-Bayes)\n",
    "  - [Stacking](#Stacking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant Main.KNeighborsClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.KNeighborsClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.SVC. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.DecisionTreeClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.KNeighborsClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.LogisticRegression. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.GaussianNB. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.MLPClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.AdaBoostClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.GradientBoostingClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.BaggingClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.StackingClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.VotingClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.RandomForestClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant Main.PCA. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generateComparisonTable (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load custom functions from provided files\n",
    "include(\"preprocessing.jl\")\n",
    "include(\"metrics.jl\")\n",
    "include(\"training.jl\")\n",
    "include(\"plotting.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "Random.seed!(42)\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"dataset.csv\"\n",
    "data = CSV.read(dataset_path, DataFrame);\n",
    "\n",
    "# Separate features and target\n",
    "target_column = :Target\n",
    "inputs = select(data, Not(target_column))\n",
    "targets = data[!, target_column];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded targets: [0, 1, 0, 1, 1]\n",
      "Decoded targets: [\"Dropout\", \"Graduate\", \"Dropout\", \"Graduate\", \"Graduate\"]\n"
     ]
    }
   ],
   "source": [
    "inputs = Float32.(Matrix(inputs))\n",
    "\n",
    "# Define the categories and their mapping\n",
    "label_mapping = Dict(\"Dropout\" => 0, \"Graduate\" => 1, \"Enrolled\" => 2)\n",
    "\n",
    "# Encode the targets\n",
    "targets_label_encoded = [label_mapping[label] for label in targets]\n",
    "\n",
    "println(\"Encoded targets: \", targets_label_encoded[1:5])\n",
    "\n",
    "# To decode later, create a reverse mapping\n",
    "reverse_mapping = Dict(v => k for (k, v) in label_mapping)\n",
    "decoded_targets = [reverse_mapping[code] for code in targets_label_encoded]\n",
    "\n",
    "println(\"Decoded targets: \", decoded_targets[1:5])\n",
    "\n",
    "# Define the number of folds for cross-validation and obtain the indices\n",
    "Random.seed!(42)\n",
    "k = 5\n",
    "N = size(inputs, 1)\n",
    "fold_indices = crossValidation(targets, k)\n",
    "metrics_to_save = [:accuracy, :precision, :recall, :f1_score];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE experiments\n",
    "\n",
    "In the first approach we see that we detected some problems in some metrics because of the imbalance of the dataset. For example, for the ANN model, we get high values of mean accuracy and low values of mean F1-score. This happens because the precision and recall of the class `Enrolled` are very low.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "<image src=\"plots/Approach1/ANN/accuracy_performance_bar.png\" width=\"600\"/>\n",
    "<image src=\"plots/Approach1/ANN/f1_score_performance_bar.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "To address this problem, we will use the SMOTE algorithm to balance the dataset. We will conduct 5 experiments:\n",
    "\n",
    "- Oversampling the minority class `Enrolled` at 200%.\n",
    "- Oversampling the minority class `Enrolled` at 300%.\n",
    "- Oversampling the minority class `Dropout` at 200% and oversampling the minority class `Enrolled` at 200%.\n",
    "- Oversampling the minority class `Dropout` at 200% and oversampling the minority class `Enrolled` at 300%.\n",
    "- Oversampling the minority class `Enrolled` at 200% and undersampling the majority class `Graduated` at 50%.\n",
    "\n",
    "We are going to fix the number of nearest neighbors to 5.\n",
    "\n",
    "To avoid retraining all the models multiple times, we will perform the experiments only with the base models: ANN, Decision Tree, SVM, and KNN, using the best hyperparameters identified in the first approach. Subsequently, we will train the models in the same manner as in the first approach, but with the balanced dataset that yielded the best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution:\n",
      "\u001b[1m3×2 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m Target   \u001b[0m\u001b[1m Count \u001b[0m\n",
      "     │\u001b[90m String15 \u001b[0m\u001b[90m Int64 \u001b[0m\n",
      "─────┼─────────────────\n",
      "   1 │ Dropout    1421\n",
      "   2 │ Graduate   2209\n",
      "   3 │ Enrolled    794\n"
     ]
    }
   ],
   "source": [
    "target_column = :Target\n",
    "println(\"\\nClass Distribution:\")\n",
    "println(combine(groupby(data, target_column), nrow => :Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nmap: Dict(\"Enrolled\" => 200)\n",
      "Number of instances: (5218,)\n",
      "Elements of class Dropout: 1421\n",
      "Elements of class Graduate: 2209\n",
      "Elements of class Enrolled: 1588\n",
      "\n",
      "Nmap: Dict(\"Enrolled\" => 300)\n",
      "Number of instances: (6012,)\n",
      "Elements of class Dropout: 1421\n",
      "Elements of class Graduate: 2209\n",
      "Elements of class Enrolled: 2382\n",
      "\n",
      "Nmap: Dict(\"Enrolled\" => 200, \"Dropout\" => 200)\n",
      "Number of instances: (6639,)\n",
      "Elements of class Dropout: 2842\n",
      "Elements of class Graduate: 2209\n",
      "Elements of class Enrolled: 1588\n",
      "\n",
      "Nmap: Dict(\"Enrolled\" => 300, \"Dropout\" => 200)\n",
      "Number of instances: (7433,)\n",
      "Elements of class Dropout: 2842\n",
      "Elements of class Graduate: 2209\n",
      "Elements of class Enrolled: 2382\n",
      "\n",
      "Nmap: Dict(\"Enrolled\" => 200, \"Graduate\" => 50)\n",
      "Number of instances: (4113,)\n",
      "Elements of class Dropout: 1421\n",
      "Elements of class Graduate: 1104\n",
      "Elements of class Enrolled: 1588\n"
     ]
    }
   ],
   "source": [
    "smote_percentages = [\n",
    "  Dict(\"Enrolled\" => 200),\n",
    "  Dict(\"Enrolled\" => 300),\n",
    "  Dict(\"Enrolled\" => 200, \"Dropout\" => 200),\n",
    "  Dict(\"Enrolled\" => 300, \"Dropout\" => 200),\n",
    "  Dict(\"Enrolled\" => 200, \"Graduate\" => 50)\n",
    "]\n",
    "k = 5\n",
    "\n",
    "open(\"warnings.log\", \"w\") do file\n",
    "  redirect_stderr(file) do # redirect warnings associated with joblib\n",
    "    for (i, smote_percentage) in enumerate(smote_percentages)\n",
    "      println(\"\\nNmap: \", smote_percentage)\n",
    "      balanced_inputs, balanced_targets = smote(inputs, targets, smote_percentage, k)\n",
    "      println(\"Number of instances: \", size(balanced_targets))\n",
    "      println(\"Elements of class Dropout: \", sum(balanced_targets .== \"Dropout\"))\n",
    "      println(\"Elements of class Graduate: \", sum(balanced_targets .== \"Graduate\"))\n",
    "      println(\"Elements of class Enrolled: \", sum(balanced_targets .== \"Enrolled\"))\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configurations\n",
    "topology = [64, 23]\n",
    "max_depth = 5\n",
    "n_neighbors = 5\n",
    "kernel = \"linear\"\n",
    "C = 10\n",
    "\n",
    "# ANN\n",
    "hyperparameters_ann = Dict(\n",
    "  \"topology\" => topology,\n",
    "  \"learningRate\" => 0.01,\n",
    "  \"maxEpochs\" => 100,\n",
    "  \"repetitionsTraining\" => 10,\n",
    "  \"validationRatio\" => 0.15,\n",
    "  \"maxEpochsVal\" => 10,\n",
    "  \"minLoss\" => 0.0001\n",
    ")\n",
    "\n",
    "# DT\n",
    "hyperparameters_dt = Dict(\n",
    "  :max_depth => max_depth,\n",
    "  :criterion => \"gini\",\n",
    "  :min_samples_split => 2,\n",
    ")\n",
    "\n",
    "# SVM\n",
    "hyperparameters_svm = Dict(\n",
    "  :kernel => kernel,\n",
    "  :C => C,\n",
    "  :gamma => \"auto\",\n",
    "  :probability => true,\n",
    ")\n",
    "\n",
    "# KNN\n",
    "hyperparameters_knn = Dict(\n",
    "  :n_neighbors => n_neighbors,\n",
    "  :weights => \"uniform\",\n",
    "  :metric => \"euclidean\",\n",
    ")\n",
    "\n",
    "# Define the hyperparameters for smote\n",
    "smote_percentages = [\n",
    "  Dict(\"Enrolled\" => 200),\n",
    "  Dict(\"Enrolled\" => 300),\n",
    "  Dict(\"Enrolled\" => 200, \"Dropout\" => 200),\n",
    "  Dict(\"Enrolled\" => 300, \"Dropout\" => 200),\n",
    "  Dict(\"Enrolled\" => 200, \"Graduate\" => 50)\n",
    "]\n",
    "k = 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_results_ann = []\n",
    "class_results_ann = []\n",
    "general_results_dt = []\n",
    "class_results_dt = []\n",
    "general_results_svm = []\n",
    "class_results_svm = []\n",
    "general_results_knn = []\n",
    "class_results_knn = []\n",
    "\n",
    "for smote_percentage in smote_percentages\n",
    "  # ANN\n",
    "  gr, cr = modelCrossValidation(\n",
    "    :ANN,\n",
    "    hyperparameters_ann,\n",
    "    inputs,\n",
    "    targets,\n",
    "    fold_indices;\n",
    "    metricsToSave=metrics_to_save,\n",
    "    normalizationType=:zeroMean,\n",
    "    applySmote=true,\n",
    "    smotePercentages=smote_percentage,\n",
    "    smoteNeighbors=k,\n",
    "    verbose=false\n",
    "  )\n",
    "  push!(general_results_ann, gr)\n",
    "  push!(class_results_ann, cr)\n",
    "\n",
    "  # DT\n",
    "  gr, cr = modelCrossValidation(\n",
    "    :DT,\n",
    "    hyperparameters_dt,\n",
    "    inputs,\n",
    "    targets,\n",
    "    fold_indices;\n",
    "    metricsToSave=metrics_to_save,\n",
    "    normalizationType=:zeroMean,\n",
    "    applySmote=true,\n",
    "    smotePercentages=smote_percentage,\n",
    "    smoteNeighbors=k,\n",
    "    verbose=false\n",
    "  )\n",
    "  push!(general_results_dt, gr)\n",
    "  push!(class_results_dt, cr)\n",
    "\n",
    "  # SVM\n",
    "  gr, cr = modelCrossValidation(\n",
    "    :SVC,\n",
    "    hyperparameters_svm,\n",
    "    inputs,\n",
    "    targets,\n",
    "    fold_indices;\n",
    "    metricsToSave=metrics_to_save,\n",
    "    normalizationType=:zeroMean,\n",
    "    applySmote=true,\n",
    "    smotePercentages=smote_percentage,\n",
    "    smoteNeighbors=k,\n",
    "    verbose=false\n",
    "  )\n",
    "  push!(general_results_svm, gr)\n",
    "  push!(class_results_svm, cr)\n",
    "\n",
    "  # KNN\n",
    "  gr, cr = modelCrossValidation(\n",
    "    :KNN,\n",
    "    hyperparameters_knn,\n",
    "    inputs,\n",
    "    targets,\n",
    "    fold_indices;\n",
    "    metricsToSave=metrics_to_save,\n",
    "    normalizationType=:zeroMean,\n",
    "    applySmote=true,\n",
    "    smotePercentages=smote_percentage,\n",
    "    smoteNeighbors=k,\n",
    "    verbose=false\n",
    "  )\n",
    "  push!(general_results_knn, gr)\n",
    "  push!(class_results_knn, cr)\n",
    "\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
